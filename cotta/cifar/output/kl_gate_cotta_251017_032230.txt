[25/10/17 03:22:30] [conf.py:  219]: PyTorch Version: torch=2.8.0+cu128, cuda=12.8, cudnn=91002
[25/10/17 03:22:30] [conf.py:  221]: BN:
  EPS: 1e-05
  MOM: 0.1
CKPT_DIR: ./ckpt
CORRUPTION:
  DATASET: cifar10
  NUM_EX: 10000
  SEVERITY: [5]
  TYPE: ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur', 'glass_blur', 'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog', 'brightness', 'contrast', 'elastic_transform', 'pixelate', 'jpeg_compression']
CUDNN:
  BENCHMARK: True
DATA_DIR: ./data
DESC: 
KL_GATE:
  THRESHOLD: 0.1
LOG_DEST: kl_gate_cotta_251017_032230.txt
LOG_TIME: 251017_032230
MODEL:
  ADAPTATION: kl_gate_cotta
  ARCH: Standard
  EPISODIC: False
OPTIM:
  AP: 0.92
  BETA: 0.9
  DAMPENING: 0.0
  LR: 0.001
  METHOD: Adam
  MOMENTUM: 0.9
  MT: 0.999
  NESTEROV: True
  RST: 0.01
  STEPS: 1
  WD: 0.0
RNG_SEED: 1
SAVE_DIR: ./output
TEST:
  BATCH_SIZE: 400
[25/10/17 03:22:34] [kl_gate_cotta.py:  161]: model for adaptation: WideResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(16, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (block2): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (block3): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
  (relu): ReLU(inplace=True)
  (fc): Linear(in_features=640, out_features=10, bias=True)
)
[25/10/17 03:22:34] [kl_gate_cotta.py:  162]: params for adaptation: ['conv1.weight', 'block1.layer.0.bn1.weight', 'block1.layer.0.bn1.bias', 'block1.layer.0.conv1.weight', 'block1.layer.0.bn2.weight', 'block1.layer.0.bn2.bias', 'block1.layer.0.conv2.weight', 'block1.layer.0.convShortcut.weight', 'block1.layer.1.bn1.weight', 'block1.layer.1.bn1.bias', 'block1.layer.1.conv1.weight', 'block1.layer.1.bn2.weight', 'block1.layer.1.bn2.bias', 'block1.layer.1.conv2.weight', 'block1.layer.2.bn1.weight', 'block1.layer.2.bn1.bias', 'block1.layer.2.conv1.weight', 'block1.layer.2.bn2.weight', 'block1.layer.2.bn2.bias', 'block1.layer.2.conv2.weight', 'block1.layer.3.bn1.weight', 'block1.layer.3.bn1.bias', 'block1.layer.3.conv1.weight', 'block1.layer.3.bn2.weight', 'block1.layer.3.bn2.bias', 'block1.layer.3.conv2.weight', 'block2.layer.0.bn1.weight', 'block2.layer.0.bn1.bias', 'block2.layer.0.conv1.weight', 'block2.layer.0.bn2.weight', 'block2.layer.0.bn2.bias', 'block2.layer.0.conv2.weight', 'block2.layer.0.convShortcut.weight', 'block2.layer.1.bn1.weight', 'block2.layer.1.bn1.bias', 'block2.layer.1.conv1.weight', 'block2.layer.1.bn2.weight', 'block2.layer.1.bn2.bias', 'block2.layer.1.conv2.weight', 'block2.layer.2.bn1.weight', 'block2.layer.2.bn1.bias', 'block2.layer.2.conv1.weight', 'block2.layer.2.bn2.weight', 'block2.layer.2.bn2.bias', 'block2.layer.2.conv2.weight', 'block2.layer.3.bn1.weight', 'block2.layer.3.bn1.bias', 'block2.layer.3.conv1.weight', 'block2.layer.3.bn2.weight', 'block2.layer.3.bn2.bias', 'block2.layer.3.conv2.weight', 'block3.layer.0.bn1.weight', 'block3.layer.0.bn1.bias', 'block3.layer.0.conv1.weight', 'block3.layer.0.bn2.weight', 'block3.layer.0.bn2.bias', 'block3.layer.0.conv2.weight', 'block3.layer.0.convShortcut.weight', 'block3.layer.1.bn1.weight', 'block3.layer.1.bn1.bias', 'block3.layer.1.conv1.weight', 'block3.layer.1.bn2.weight', 'block3.layer.1.bn2.bias', 'block3.layer.1.conv2.weight', 'block3.layer.2.bn1.weight', 'block3.layer.2.bn1.bias', 'block3.layer.2.conv1.weight', 'block3.layer.2.bn2.weight', 'block3.layer.2.bn2.bias', 'block3.layer.2.conv2.weight', 'block3.layer.3.bn1.weight', 'block3.layer.3.bn1.bias', 'block3.layer.3.conv1.weight', 'block3.layer.3.bn2.weight', 'block3.layer.3.bn2.bias', 'block3.layer.3.conv2.weight', 'bn1.weight', 'bn1.bias', 'fc.weight', 'fc.bias']
[25/10/17 03:22:34] [kl_gate_cotta.py:  163]: optimizer for adaptation: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
[25/10/17 03:22:34] [kl_gate_cotta.py:  164]: KL threshold: 1.0
[25/10/17 03:22:34] [cifar10c_KL.py:   74]: Running KL-Gate CoTTA with threshold: 1.0
[25/10/17 03:22:34] [cifar10c_KL.py:   94]: resetting model
[25/10/17 03:23:18] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.4028 > threshold=1.0000, skipping update
[25/10/17 03:23:25] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.4362 > threshold=1.0000, skipping update
[25/10/17 03:23:32] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.7927 > threshold=1.0000, skipping update
[25/10/17 03:23:39] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.5403 > threshold=1.0000, skipping update
[25/10/17 03:23:46] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.5281 > threshold=1.0000, skipping update
[25/10/17 03:23:53] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.7769 > threshold=1.0000, skipping update
[25/10/17 03:24:01] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.6140 > threshold=1.0000, skipping update
[25/10/17 03:24:07] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.8542 > threshold=1.0000, skipping update
[25/10/17 03:24:14] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.2999 > threshold=1.0000, skipping update
[25/10/17 03:24:22] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.7858 > threshold=1.0000, skipping update
[25/10/17 03:24:29] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.8167 > threshold=1.0000, skipping update
[25/10/17 03:24:36] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.5791 > threshold=1.0000, skipping update
[25/10/17 03:24:43] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.6061 > threshold=1.0000, skipping update
[25/10/17 03:24:50] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.6950 > threshold=1.0000, skipping update
[25/10/17 03:24:57] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.6563 > threshold=1.0000, skipping update
[25/10/17 03:25:04] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.5348 > threshold=1.0000, skipping update
[25/10/17 03:25:11] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.6580 > threshold=1.0000, skipping update
[25/10/17 03:25:18] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.7313 > threshold=1.0000, skipping update
[25/10/17 03:25:26] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.6293 > threshold=1.0000, skipping update
[25/10/17 03:25:33] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.7509 > threshold=1.0000, skipping update
[25/10/17 03:25:40] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.7865 > threshold=1.0000, skipping update
[25/10/17 03:25:47] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.6088 > threshold=1.0000, skipping update
[25/10/17 03:25:55] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.5286 > threshold=1.0000, skipping update
[25/10/17 03:25:55] [cifar10c_KL.py:  127]: Corruption: gaussian_noise5, Error: 24.62%, Updates: 2/25 (8.0%)
[25/10/17 03:25:55] [cifar10c_KL.py:   98]: not resetting model
[25/10/17 03:26:02] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.4490 > threshold=1.0000, skipping update
[25/10/17 03:26:09] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.6408 > threshold=1.0000, skipping update
[25/10/17 03:26:17] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.2764 > threshold=1.0000, skipping update
[25/10/17 03:26:23] [kl_gate_cotta.py:  102]: KL-Gate triggered: KL=2.6094 > threshold=1.0000, skipping update
